<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-02-08T15:48:54+01:00</updated><id>/feed.xml</id><title type="html">Bernat Font</title><subtitle>Dr. Bernat Font's homepage
</subtitle><author><name>Bernat Font</name><email>bernatfontgarcia@gmail.com</email></author><entry><title type="html">Porting WaterLily.jl into a backend-agnostic solver</title><link href="/2023/05/07/waterlily-on-gpu.html" rel="alternate" type="text/html" title="Porting WaterLily.jl into a backend-agnostic solver" /><published>2023-05-07T15:00:00+02:00</published><updated>2023-05-07T15:00:00+02:00</updated><id>/2023/05/07/waterlily-on-gpu</id><content type="html" xml:base="/2023/05/07/waterlily-on-gpu.html"><![CDATA[<p><a href="https://github.com/weymouth/WaterLily.jl">WaterLily.jl</a> is a simple and fast fluid simulator written in pure Julia. It solves the unsteady incompressible 2D or 3D <a href="https://en.wikipedia.org/wiki/Navier%E2%80%93Stokes_equations">Navier-Stokes equations</a> on a Cartesian grid.
The pressure Poisson equation is solved with a <a href="https://en.wikipedia.org/wiki/Multigrid_method">geometric multigrid</a> method.
Solid boundaries are modelled using the <a href="https://eprints.soton.ac.uk/369635/">Boundary Data Immersion Method</a>.</p>

<p><a href="https://github.com/weymouth/WaterLily.jl/releases/tag/v1.0.0">v1.0</a> has ported the solver from a serial CPU execution to a backend-agnostic execution including multi-threaded CPU and GPU from different vendors (NVIDIA and AMD) thanks to <a href="https://github.com/JuliaGPU/KernelAbstractions.jl">KernelAbstractions.jl</a> (KA).
We have also recently published an extended abstract preprint with benchmarking details regarding this port (see <a href="https://arxiv.org/abs/2304.08159">arXiv</a>).
In this post, we will review our approach to port the code together with the challenges we have faced.</p>

<h3 id="introducing-kernelabstractionsjl">Introducing KernelAbstractions.jl</h3>

<p>The main ingredient of this port is the <a href="https://juliagpu.gitlab.io/KernelAbstractions.jl/api/#KernelAbstractions.@kernel"><code class="language-plaintext highlighter-rouge">@kernel</code></a> macro from KA.
This macro takes a function definition and converts it into a kernel specialised for a given backend. KA can work with CUDA, ROCm, oneAPI, and Metal backends.</p>

<p>As example, consider the divergence operator for a 2D vector field $\vec{u}=(u, v)$.
In the finite-volume method (FVM) and using a Cartesian (uniform) grid with unit cells, this is defined as</p>

\[\begin{align}
\sigma=\unicode{x2230}(\nabla\cdot\vec{u})\,\mathrm{d}V = \unicode{x222F}\vec{u}\cdot\hat{n}\,\mathrm{d}S\rightarrow \sigma_{i,j} = (u_{i+1,j} - u_{i,j}) + (v_{i,j+1} - v_{i,j}),
\end{align}\]

<p>where $i$ and $j$ are the indices of the discretised staggered grid:</p>

<p class="text-align-center"><img src="/assets/images/2023-07-05-waterlily-on-gpu/divergence.svg#center" alt="staggered grid" width="40%" class="img-responsive" /></p>

<p>In WaterLily, we define loops based on the <code class="language-plaintext highlighter-rouge">CartesianIndex</code> such that <code class="language-plaintext highlighter-rouge">I=(i,j,...)</code>, thus writing an n-dimensional solver in a very straight-forward way.
With this, to compute the divergence of a 2D vector field we can use</p>
<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">δ</span><span class="x">(</span><span class="n">d</span><span class="x">,</span><span class="o">::</span><span class="kt">CartesianIndex</span><span class="x">{</span><span class="n">D</span><span class="x">})</span> <span class="k">where</span> <span class="x">{</span><span class="n">D</span><span class="x">}</span> <span class="o">=</span> <span class="kt">CartesianIndex</span><span class="x">(</span><span class="n">ntuple</span><span class="x">(</span><span class="n">j</span> <span class="o">-&gt;</span> <span class="n">j</span><span class="o">==</span><span class="n">d</span> <span class="o">?</span> <span class="mi">1</span> <span class="o">:</span> <span class="mi">0</span><span class="x">,</span> <span class="n">D</span><span class="x">))</span>
<span class="nd">@inline</span> <span class="n">∂</span><span class="x">(</span><span class="n">a</span><span class="x">,</span><span class="n">I</span><span class="o">::</span><span class="kt">CartesianIndex</span><span class="x">{</span><span class="n">D</span><span class="x">},</span><span class="n">u</span><span class="o">::</span><span class="kt">AbstractArray</span><span class="x">{</span><span class="n">T</span><span class="x">,</span><span class="n">n</span><span class="x">})</span> <span class="k">where</span> <span class="x">{</span><span class="n">D</span><span class="x">,</span><span class="n">T</span><span class="x">,</span><span class="n">n</span><span class="x">}</span> <span class="o">=</span> <span class="n">u</span><span class="x">[</span><span class="n">I</span><span class="o">+</span><span class="n">δ</span><span class="x">(</span><span class="n">a</span><span class="x">,</span><span class="n">I</span><span class="x">),</span><span class="n">a</span><span class="x">]</span> <span class="o">-</span> <span class="n">u</span><span class="x">[</span><span class="n">I</span><span class="x">,</span><span class="n">a</span><span class="x">]</span>
<span class="n">inside</span><span class="x">(</span><span class="n">a</span><span class="x">)</span> <span class="o">=</span> <span class="kt">CartesianIndices</span><span class="x">(</span><span class="n">ntuple</span><span class="x">(</span><span class="n">i</span><span class="o">-&gt;</span> <span class="mi">2</span><span class="o">:</span><span class="n">size</span><span class="x">(</span><span class="n">a</span><span class="x">)[</span><span class="n">i</span><span class="x">]</span><span class="o">-</span><span class="mi">1</span><span class="x">,</span> <span class="n">ndims</span><span class="x">(</span><span class="n">a</span><span class="x">)))</span>

<span class="n">N</span> <span class="o">=</span> <span class="x">(</span><span class="mi">10</span><span class="x">,</span> <span class="mi">10</span><span class="x">)</span> <span class="c"># domain size</span>
<span class="n">σ</span> <span class="o">=</span> <span class="n">zeros</span><span class="x">(</span><span class="n">N</span><span class="x">)</span> <span class="c"># scalar field</span>
<span class="n">u</span> <span class="o">=</span> <span class="n">rand</span><span class="x">(</span><span class="n">N</span><span class="o">...</span><span class="x">,</span> <span class="n">length</span><span class="x">(</span><span class="n">N</span><span class="x">))</span> <span class="c"># 2D vector field with ghost cells</span>

<span class="k">for</span> <span class="n">d</span> <span class="n">∈</span> <span class="mi">1</span><span class="o">:</span><span class="n">ndims</span><span class="x">(</span><span class="n">σ</span><span class="x">),</span> <span class="n">I</span> <span class="n">∈</span> <span class="n">inside</span><span class="x">(</span><span class="n">σ</span><span class="x">)</span>
    <span class="n">σ</span><span class="x">[</span><span class="n">I</span><span class="x">]</span> <span class="o">+=</span> <span class="n">∂</span><span class="x">(</span><span class="n">d</span><span class="x">,</span> <span class="n">I</span><span class="x">,</span> <span class="n">u</span><span class="x">)</span>
<span class="k">end</span>
</code></pre></div></div>
<p>where a loop for each dimension <code class="language-plaintext highlighter-rouge">d</code> and each Cartesian index <code class="language-plaintext highlighter-rouge">I</code> is used.
The function <code class="language-plaintext highlighter-rouge">δ</code> provides a Cartesian step in the direction <code class="language-plaintext highlighter-rouge">d</code>, for example <code class="language-plaintext highlighter-rouge">δ(1, 2)</code> returns <code class="language-plaintext highlighter-rouge">CartesianIndex(1, 0)</code> and <code class="language-plaintext highlighter-rouge">δ(2, 3)</code> returns <code class="language-plaintext highlighter-rouge">CartesianIndex(0, 1, 0)</code>.
This is used in the derivative function <code class="language-plaintext highlighter-rouge">∂</code> to implement the divergence equation as described above.
<code class="language-plaintext highlighter-rouge">inside(σ)</code> provides the <code class="language-plaintext highlighter-rouge">CartesianIndices</code> of <code class="language-plaintext highlighter-rouge">σ</code> excluding the ghost elements, <em>ie.</em> a range of Cartesian indices starting at <code class="language-plaintext highlighter-rouge">(2, 2)</code> and ending at <code class="language-plaintext highlighter-rouge">(9, 9)</code> when <code class="language-plaintext highlighter-rouge">size(σ) == (10, 10)</code>.</p>

<p>Note that the divergence operation is independent for each <code class="language-plaintext highlighter-rouge">I</code>, and this is great because it means we can parallelize it!
This is where KA comes into place.
To be able to generate the divergence operator using KA, we need to write the divergence kernel which is just the divergence operator written in KA style.</p>

<p>We define the divergence kernel <code class="language-plaintext highlighter-rouge">_divergence!</code> and a wrapper <code class="language-plaintext highlighter-rouge">divergence!</code> as follows</p>
<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">using</span> <span class="n">KernelAbstractions</span><span class="o">:</span> <span class="n">get_backend</span><span class="x">,</span> <span class="nd">@index</span><span class="x">,</span> <span class="nd">@kernel</span>

<span class="nd">@kernel</span> <span class="k">function</span><span class="nf"> _divergence!</span><span class="x">(</span><span class="n">σ</span><span class="x">,</span> <span class="n">u</span><span class="x">,</span> <span class="nd">@Const</span><span class="x">(</span><span class="n">I0</span><span class="x">))</span>
    <span class="n">I</span> <span class="o">=</span> <span class="nd">@index</span><span class="x">(</span><span class="n">Global</span><span class="x">,</span> <span class="n">Cartesian</span><span class="x">)</span>
    <span class="n">I</span> <span class="o">+=</span> <span class="n">I0</span>
    <span class="n">σ_sum</span> <span class="o">=</span> <span class="n">zero</span><span class="x">(</span><span class="n">eltype</span><span class="x">(</span><span class="n">σ</span><span class="x">))</span>
    <span class="k">for</span> <span class="n">d</span> <span class="n">∈</span> <span class="mi">1</span><span class="o">:</span><span class="n">ndims</span><span class="x">(</span><span class="n">σ</span><span class="x">)</span>
        <span class="n">σ_sum</span> <span class="o">+=</span> <span class="n">∂</span><span class="x">(</span><span class="n">d</span><span class="x">,</span> <span class="n">I</span><span class="x">,</span> <span class="n">u</span><span class="x">)</span>
    <span class="k">end</span>
    <span class="n">σ</span><span class="x">[</span><span class="n">I</span><span class="x">]</span> <span class="o">=</span> <span class="n">σ_sum</span>
<span class="k">end</span>
<span class="k">function</span><span class="nf"> divergence!</span><span class="x">(</span><span class="n">σ</span><span class="x">,</span> <span class="n">u</span><span class="x">)</span>
    <span class="n">R</span> <span class="o">=</span> <span class="n">inside</span><span class="x">(</span><span class="n">σ</span><span class="x">)</span>
    <span class="n">_divergence!</span><span class="x">(</span><span class="n">get_backend</span><span class="x">(</span><span class="n">σ</span><span class="x">),</span> <span class="mi">64</span><span class="x">)(</span><span class="n">σ</span><span class="x">,</span> <span class="n">u</span><span class="x">,</span> <span class="n">R</span><span class="x">[</span><span class="mi">1</span><span class="x">]</span><span class="o">-</span><span class="n">oneunit</span><span class="x">(</span><span class="n">R</span><span class="x">[</span><span class="mi">1</span><span class="x">]),</span> <span class="n">ndrange</span><span class="o">=</span><span class="n">size</span><span class="x">(</span><span class="n">R</span><span class="x">))</span>
<span class="k">end</span>
</code></pre></div></div>
<p>Note that in the <code class="language-plaintext highlighter-rouge">_divergence!</code> kernel we operate again using Cartesian indices by calling <code class="language-plaintext highlighter-rouge">@index(Global, Cartesian)</code> from the KA <a href="https://juliagpu.github.io/KernelAbstractions.jl/stable/api/#KernelAbstractions.@index"><code class="language-plaintext highlighter-rouge">@index</code></a> macro.
The range of Cartesian indices is given by the <code class="language-plaintext highlighter-rouge">ndrange</code> argument in the wrapper function where we pass the <code class="language-plaintext highlighter-rouge">inside(σ)</code> Cartesian indices range, and the backend is inferred with the <code class="language-plaintext highlighter-rouge">get_backend</code> method.
Also note that we pass an additional (constant) argument <code class="language-plaintext highlighter-rouge">I0</code> which provides the offset index to apply to the indices given by <code class="language-plaintext highlighter-rouge">@index</code> naturally starting on <code class="language-plaintext highlighter-rouge">(1,1,...)</code>.
Using a workgroup size of <code class="language-plaintext highlighter-rouge">64</code> (size of the group of threads acting in parallel, see <a href="https://juliagpu.github.io/KernelAbstractions.jl/stable/quickstart/#Terminology-1">terminology</a>), KA will parallelize over <code class="language-plaintext highlighter-rouge">I</code> by multi-threading in CPU or GPU.
In this regard, we just need to change the array type of <code class="language-plaintext highlighter-rouge">σ</code> and <code class="language-plaintext highlighter-rouge">u</code> from <code class="language-plaintext highlighter-rouge">Array</code> (CPU backend) to <code class="language-plaintext highlighter-rouge">CuArray</code> (NVIDIA GPU backend) or <code class="language-plaintext highlighter-rouge">ROCArray</code> (AMD GPU backend), and KA will specialise the kernel for the desired backend</p>
<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">using</span> <span class="n">CUDA</span><span class="o">:</span> <span class="n">CuArray</span>

<span class="n">N</span> <span class="o">=</span> <span class="x">(</span><span class="mi">10</span><span class="x">,</span> <span class="mi">10</span><span class="x">)</span>
<span class="n">σ</span> <span class="o">=</span> <span class="n">zeros</span><span class="x">(</span><span class="n">N</span><span class="x">)</span> <span class="o">|&gt;</span> <span class="n">CuArray</span>
<span class="n">u</span> <span class="o">=</span> <span class="n">rand</span><span class="x">(</span><span class="n">N</span><span class="o">...</span><span class="x">,</span> <span class="n">length</span><span class="x">(</span><span class="n">N</span><span class="x">))</span> <span class="o">|&gt;</span> <span class="n">CuArray</span>

<span class="n">divergence!</span><span class="x">(</span><span class="n">σ</span><span class="x">,</span> <span class="n">u</span><span class="x">)</span>
</code></pre></div></div>

<h3 id="automatic-loop-and-kernel-generation">Automatic loop and kernel generation</h3>

<p>As a stencil-based CFD solver, WaterLily heavily uses <code class="language-plaintext highlighter-rouge">for</code> loops to iterate over the n-dimensional arrays.
To automate the generation of such loops, the macro <code class="language-plaintext highlighter-rouge">@loop</code> is defined</p>
<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">macro</span><span class="nf"> loop</span><span class="x">(</span><span class="n">args</span><span class="o">...</span><span class="x">)</span>
    <span class="n">ex</span><span class="x">,</span><span class="n">_</span><span class="x">,</span><span class="n">itr</span> <span class="o">=</span> <span class="n">args</span>
    <span class="n">op</span><span class="x">,</span><span class="n">I</span><span class="x">,</span><span class="n">R</span> <span class="o">=</span> <span class="n">itr</span><span class="o">.</span><span class="n">args</span>
    <span class="nd">@assert</span> <span class="n">op</span> <span class="n">∈</span> <span class="x">(</span><span class="o">:</span><span class="x">(</span><span class="n">∈</span><span class="x">),</span><span class="o">:</span><span class="x">(</span><span class="k">in</span><span class="x">))</span>
    <span class="k">return</span> <span class="k">quote</span>
        <span class="k">for</span> <span class="o">$</span><span class="n">I</span> <span class="n">∈</span> <span class="o">$</span><span class="n">R</span>
            <span class="o">$</span><span class="n">ex</span>
        <span class="k">end</span>
    <span class="k">end</span> <span class="o">|&gt;</span> <span class="n">esc</span>
<span class="k">end</span>
</code></pre></div></div>
<p>This macro takes an expression such as <code class="language-plaintext highlighter-rouge">@loop &lt;expr&gt; over I ∈ R</code> where <code class="language-plaintext highlighter-rouge">R</code> is a <code class="language-plaintext highlighter-rouge">CartesianIndices</code> range, and produces the loop <code class="language-plaintext highlighter-rouge">for I ∈ R &lt;expr&gt; end</code>.
For example, the serial divergence operator could now be simply defined using</p>
<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">d</span> <span class="n">∈</span> <span class="mi">1</span><span class="o">:</span><span class="n">ndims</span><span class="x">(</span><span class="n">σ</span><span class="x">)</span>
    <span class="nd">@loop</span> <span class="n">σ</span><span class="x">[</span><span class="n">I</span><span class="x">]</span> <span class="o">+=</span> <span class="n">∂</span><span class="x">(</span><span class="n">d</span><span class="x">,</span> <span class="n">I</span><span class="x">,</span> <span class="n">u</span><span class="x">)</span> <span class="n">over</span> <span class="n">I</span> <span class="n">∈</span> <span class="n">inside</span><span class="x">(</span><span class="n">σ</span><span class="x">)</span>
<span class="k">end</span>
</code></pre></div></div>
<p>which generates the <code class="language-plaintext highlighter-rouge">I ∈ inside(σ)</code> loop automatically.</p>

<p>Even though this could be seen as small improvement (if any), the nice thing about writing loops using this approach is that the computationally-demanding part of the code can be abstracted out of the main workflow.
For example, it is easy to add performance macros such as <code class="language-plaintext highlighter-rouge">@inbounds</code> and/or <code class="language-plaintext highlighter-rouge">@fastmath</code> to each loop by changing the quote block in the <code class="language-plaintext highlighter-rouge">@loop</code> macro</p>
<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">macro</span><span class="nf"> loop</span><span class="x">(</span><span class="n">args</span><span class="o">...</span><span class="x">)</span>
    <span class="n">ex</span><span class="x">,</span><span class="n">_</span><span class="x">,</span><span class="n">itr</span> <span class="o">=</span> <span class="n">args</span>
    <span class="n">op</span><span class="x">,</span><span class="n">I</span><span class="x">,</span><span class="n">R</span> <span class="o">=</span> <span class="n">itr</span><span class="o">.</span><span class="n">args</span>
    <span class="nd">@assert</span> <span class="n">op</span> <span class="n">∈</span> <span class="x">(</span><span class="o">:</span><span class="x">(</span><span class="n">∈</span><span class="x">),</span><span class="o">:</span><span class="x">(</span><span class="k">in</span><span class="x">))</span>
    <span class="k">return</span> <span class="k">quote</span>
        <span class="nd">@inbounds</span> <span class="k">for</span> <span class="o">$</span><span class="n">I</span> <span class="n">∈</span> <span class="o">$</span><span class="n">R</span>
            <span class="nd">@fastmath</span> <span class="o">$</span><span class="n">ex</span>
        <span class="k">end</span>
    <span class="k">end</span> <span class="o">|&gt;</span> <span class="n">esc</span>
<span class="k">end</span>
</code></pre></div></div>
<p>And, even nicer, we can also use this approach to automatically generate KA kernels for every loop in the code!
To do so, we modify <code class="language-plaintext highlighter-rouge">@loop</code> to generate the KA kernel using <code class="language-plaintext highlighter-rouge">@kernel</code> and the wrapper function that sets the backend and the workgroup size</p>
<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">macro</span><span class="nf"> loop</span><span class="x">(</span><span class="n">args</span><span class="o">...</span><span class="x">)</span>
    <span class="n">ex</span><span class="x">,</span><span class="n">_</span><span class="x">,</span><span class="n">itr</span> <span class="o">=</span> <span class="n">args</span>
    <span class="n">_</span><span class="x">,</span><span class="n">I</span><span class="x">,</span><span class="n">R</span> <span class="o">=</span> <span class="n">itr</span><span class="o">.</span><span class="n">args</span><span class="x">;</span> <span class="n">sym</span> <span class="o">=</span> <span class="x">[]</span>
    <span class="n">grab!</span><span class="x">(</span><span class="n">sym</span><span class="x">,</span><span class="n">ex</span><span class="x">)</span>     <span class="c"># get arguments and replace composites in `ex`</span>
    <span class="n">setdiff!</span><span class="x">(</span><span class="n">sym</span><span class="x">,[</span><span class="n">I</span><span class="x">])</span> <span class="c"># don't want to pass I as an argument</span>
    <span class="nd">@gensym</span> <span class="n">kern</span>      <span class="c"># generate unique kernel function name</span>
    <span class="k">return</span> <span class="k">quote</span>
        <span class="nd">@kernel</span> <span class="k">function</span><span class="nf"> $kern</span><span class="x">(</span><span class="o">$</span><span class="x">(</span><span class="n">rep</span><span class="o">.</span><span class="x">(</span><span class="n">sym</span><span class="x">)</span><span class="o">...</span><span class="x">),</span><span class="nd">@Const</span><span class="x">(</span><span class="n">I0</span><span class="x">))</span> <span class="c"># replace composite arguments</span>
            <span class="o">$</span><span class="n">I</span> <span class="o">=</span> <span class="nd">@index</span><span class="x">(</span><span class="n">Global</span><span class="x">,</span><span class="n">Cartesian</span><span class="x">)</span>
            <span class="o">$</span><span class="n">I</span> <span class="o">+=</span> <span class="n">I0</span>
            <span class="o">$</span><span class="n">ex</span>
        <span class="k">end</span>
        <span class="o">$</span><span class="n">kern</span><span class="x">(</span><span class="n">get_backend</span><span class="x">(</span><span class="o">$</span><span class="x">(</span><span class="n">sym</span><span class="x">[</span><span class="mi">1</span><span class="x">])),</span><span class="mi">64</span><span class="x">)(</span><span class="o">$</span><span class="x">(</span><span class="n">sym</span><span class="o">...</span><span class="x">),</span><span class="o">$</span><span class="n">R</span><span class="x">[</span><span class="mi">1</span><span class="x">]</span><span class="o">-</span><span class="n">oneunit</span><span class="x">(</span><span class="o">$</span><span class="n">R</span><span class="x">[</span><span class="mi">1</span><span class="x">]),</span><span class="n">ndrange</span><span class="o">=</span><span class="n">size</span><span class="x">(</span><span class="o">$</span><span class="n">R</span><span class="x">))</span>
    <span class="k">end</span> <span class="o">|&gt;</span> <span class="n">esc</span>
<span class="k">end</span>
<span class="k">function</span><span class="nf"> grab!</span><span class="x">(</span><span class="n">sym</span><span class="x">,</span><span class="n">ex</span><span class="o">::</span><span class="kt">Expr</span><span class="x">)</span>
    <span class="n">ex</span><span class="o">.</span><span class="n">head</span> <span class="o">==</span> <span class="o">:.</span> <span class="o">&amp;&amp;</span> <span class="k">return</span> <span class="n">union!</span><span class="x">(</span><span class="n">sym</span><span class="x">,[</span><span class="n">ex</span><span class="x">])</span>      <span class="c"># grab composite name and return</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">ex</span><span class="o">.</span><span class="n">head</span><span class="o">==:</span><span class="x">(</span><span class="n">call</span><span class="x">)</span> <span class="o">?</span> <span class="mi">2</span> <span class="o">:</span> <span class="mi">1</span>              <span class="c"># don't grab function names</span>
    <span class="n">foreach</span><span class="x">(</span><span class="n">a</span><span class="o">-&gt;</span><span class="n">grab!</span><span class="x">(</span><span class="n">sym</span><span class="x">,</span><span class="n">a</span><span class="x">),</span><span class="n">ex</span><span class="o">.</span><span class="n">args</span><span class="x">[</span><span class="n">start</span><span class="o">:</span><span class="k">end</span><span class="x">])</span>   <span class="c"># recurse into args</span>
    <span class="n">ex</span><span class="o">.</span><span class="n">args</span><span class="x">[</span><span class="n">start</span><span class="o">:</span><span class="k">end</span><span class="x">]</span> <span class="o">=</span> <span class="n">rep</span><span class="o">.</span><span class="x">(</span><span class="n">ex</span><span class="o">.</span><span class="n">args</span><span class="x">[</span><span class="n">start</span><span class="o">:</span><span class="k">end</span><span class="x">])</span> <span class="c"># replace composites in args</span>
<span class="k">end</span>
<span class="n">grab!</span><span class="x">(</span><span class="n">sym</span><span class="x">,</span><span class="n">ex</span><span class="o">::</span><span class="kt">Symbol</span><span class="x">)</span> <span class="o">=</span> <span class="n">union!</span><span class="x">(</span><span class="n">sym</span><span class="x">,[</span><span class="n">ex</span><span class="x">])</span>        <span class="c"># grab symbol name</span>
<span class="n">grab!</span><span class="x">(</span><span class="n">sym</span><span class="x">,</span><span class="n">ex</span><span class="x">)</span> <span class="o">=</span> <span class="nb">nothing</span>
<span class="n">rep</span><span class="x">(</span><span class="n">ex</span><span class="x">)</span> <span class="o">=</span> <span class="n">ex</span>
<span class="n">rep</span><span class="x">(</span><span class="n">ex</span><span class="o">::</span><span class="kt">Expr</span><span class="x">)</span> <span class="o">=</span> <span class="n">ex</span><span class="o">.</span><span class="n">head</span> <span class="o">==</span> <span class="o">:.</span> <span class="o">?</span> <span class="kt">Symbol</span><span class="x">(</span><span class="n">ex</span><span class="o">.</span><span class="n">args</span><span class="x">[</span><span class="mi">2</span><span class="x">]</span><span class="o">.</span><span class="n">value</span><span class="x">)</span> <span class="o">:</span> <span class="n">ex</span>
</code></pre></div></div>

<p>The helper functions <code class="language-plaintext highlighter-rouge">grab!</code> and <code class="language-plaintext highlighter-rouge">rep</code> allow to extract the arguments required by the expression <code class="language-plaintext highlighter-rouge">ex</code> and the Cartesian index range that will be passed to the kernel.</p>

<p>The code generated by <code class="language-plaintext highlighter-rouge">@loop</code> and <code class="language-plaintext highlighter-rouge">@kernel</code> can be explored using <code class="language-plaintext highlighter-rouge">@macroexpand</code>. For example, for <code class="language-plaintext highlighter-rouge">d=1</code></p>
<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@macroexpand</span> <span class="nd">@loop</span> <span class="n">σ</span><span class="x">[</span><span class="n">I</span><span class="x">]</span> <span class="o">+=</span> <span class="n">∂</span><span class="x">(</span><span class="mi">1</span><span class="x">,</span> <span class="n">I</span><span class="x">,</span> <span class="n">u</span><span class="x">)</span> <span class="n">over</span> <span class="n">I</span> <span class="n">∈</span> <span class="n">inside</span><span class="x">(</span><span class="n">σ</span><span class="x">)</span>
</code></pre></div></div>
<p>we can observe that the code for both CPU and GPU kernels is produced:</p>
<details>
<summary>Generated code</summary>
<pre>
@macroexpand @loopKA σ[I] += ∂(1, I, u) over I ∈ inside(σ)
quote
    begin
        function var"cpu_##kern#339"(__ctx__, σ, u, I0; )
            let I0 = (KernelAbstractions.constify)(I0)
                $(Expr(:aliasscope))
                begin
                    var"##N#341" = length((KernelAbstractions.__workitems_iterspace)(__ctx__))
                    begin
                        for var"##I#340" = (KernelAbstractions.__workitems_iterspace)(__ctx__)
                            (KernelAbstractions.__validindex)(__ctx__, var"##I#340") || continue
                            I = KernelAbstractions.__index_Global_Cartesian(__ctx__, var"##I#340")
                            begin
                                I += I0
                                σ[I] += ∂(1, I, u)
                            end
                        end
                    end
                end
                $(Expr(:popaliasscope))
                return nothing
            end
        end
        function var"gpu_##kern#339"(__ctx__, σ, u, I0; )
            let I0 = (KernelAbstractions.constify)(I0)
                if (KernelAbstractions.__validindex)(__ctx__)
                    begin
                        I = KernelAbstractions.__index_Global_Cartesian(__ctx__)
                        I += I0
                        σ[I] += ∂(1, I, u)
                    end
                end
                return nothing
            end
        end
        begin
            if !($(Expr(:isdefined, Symbol("##kern#339"))))
                begin
                    $(Expr(:meta, :doc))
                    var"##kern#339"(dev) = begin
                            var"##kern#339"(dev, (KernelAbstractions.NDIteration.DynamicSize)(), (KernelAbstractions.NDIteration.DynamicSize)())
                        end
                end
                var"##kern#339"(dev, size) = begin
                        var"##kern#339"(dev, (KernelAbstractions.NDIteration.StaticSize)(size), (KernelAbstractions.NDIteration.DynamicSize)())
                    end
                var"##kern#339"(dev, size, range) = begin
                        var"##kern#339"(dev, (KernelAbstractions.NDIteration.StaticSize)(size), (KernelAbstractions.NDIteration.StaticSize)(range))
                    end
                function var"##kern#339"(dev::Dev, sz::S, range::NDRange) where {Dev, S &lt;: KernelAbstractions.NDIteration._Size, NDRange &lt;: KernelAbstractions.NDIteration._Size}
                    if (KernelAbstractions.isgpu)(dev)
                        return (KernelAbstractions.construct)(dev, sz, range, var"gpu_##kern#339")
                    else
                        return (KernelAbstractions.construct)(dev, sz, range, var"cpu_##kern#339")
                    end
                end
            end
        end
    end
    (var"##kern#339"(get_backend(σ), 64))(σ, u, (inside(σ))[1] - oneunit((inside(σ))[1]), ndrange = size(inside(σ)))
end
</pre></details>

<p>The best feature we achieve when modifying <code class="language-plaintext highlighter-rouge">@loop</code> to produce KA kernels is that the divergence operator remains the same as before using KA</p>
<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">d</span> <span class="n">∈</span> <span class="mi">1</span><span class="o">:</span><span class="n">ndims</span><span class="x">(</span><span class="n">σ</span><span class="x">)</span>
    <span class="nd">@loop</span> <span class="n">σ</span><span class="x">[</span><span class="n">I</span><span class="x">]</span> <span class="o">+=</span> <span class="n">∂</span><span class="x">(</span><span class="n">d</span><span class="x">,</span> <span class="n">I</span><span class="x">,</span> <span class="n">u</span><span class="x">)</span> <span class="n">over</span> <span class="n">I</span> <span class="n">∈</span> <span class="n">inside</span><span class="x">(</span><span class="n">σ</span><span class="x">)</span>
<span class="k">end</span>
</code></pre></div></div>
<p>This exact approach is what has allowed WaterLily to have the same LOC as before using KA, just around 800!</p>

<h3 id="benchmarking">Benchmarking</h3>

<p>Now that we have all the items in place, we can benchmark the speedup achieved by KA compared to the serial execution using <a href="https://github.com/JuliaCI/BenchmarkTools.jl">BenchmarkTools.jl</a>.
Let’s now gather all the code we have used and create a small benchmarking MWE (see below or <a href="/assets/codes/WaterLily_on_GPU.zip">download it here</a>).
In this code showcase, we will refer to the serial CPU execution as “serial”, the multi-threaded CPU execution as “CPU”, and the GPU execution as “GPU”:</p>
<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">using</span> <span class="n">KernelAbstractions</span><span class="o">:</span> <span class="n">get_backend</span><span class="x">,</span> <span class="n">synchronize</span><span class="x">,</span> <span class="nd">@index</span><span class="x">,</span> <span class="nd">@kernel</span><span class="x">,</span> <span class="nd">@groupsize</span>
<span class="k">using</span> <span class="n">CUDA</span><span class="o">:</span> <span class="n">CuArray</span>
<span class="k">using</span> <span class="n">BenchmarkTools</span>

<span class="n">δ</span><span class="x">(</span><span class="n">d</span><span class="x">,</span><span class="o">::</span><span class="kt">CartesianIndex</span><span class="x">{</span><span class="n">D</span><span class="x">})</span> <span class="k">where</span> <span class="x">{</span><span class="n">D</span><span class="x">}</span> <span class="o">=</span> <span class="kt">CartesianIndex</span><span class="x">(</span><span class="n">ntuple</span><span class="x">(</span><span class="n">j</span> <span class="o">-&gt;</span> <span class="n">j</span><span class="o">==</span><span class="n">d</span> <span class="o">?</span> <span class="mi">1</span> <span class="o">:</span> <span class="mi">0</span><span class="x">,</span> <span class="n">D</span><span class="x">))</span>
<span class="nd">@inline</span> <span class="n">∂</span><span class="x">(</span><span class="n">a</span><span class="x">,</span><span class="n">I</span><span class="o">::</span><span class="kt">CartesianIndex</span><span class="x">{</span><span class="n">D</span><span class="x">},</span><span class="n">u</span><span class="o">::</span><span class="kt">AbstractArray</span><span class="x">{</span><span class="n">T</span><span class="x">,</span><span class="n">n</span><span class="x">})</span> <span class="k">where</span> <span class="x">{</span><span class="n">D</span><span class="x">,</span><span class="n">T</span><span class="x">,</span><span class="n">n</span><span class="x">}</span> <span class="o">=</span> <span class="n">u</span><span class="x">[</span><span class="n">I</span><span class="o">+</span><span class="n">δ</span><span class="x">(</span><span class="n">a</span><span class="x">,</span><span class="n">I</span><span class="x">),</span><span class="n">a</span><span class="x">]</span><span class="o">-</span><span class="n">u</span><span class="x">[</span><span class="n">I</span><span class="x">,</span><span class="n">a</span><span class="x">]</span>
<span class="n">inside</span><span class="x">(</span><span class="n">a</span><span class="x">)</span> <span class="o">=</span> <span class="kt">CartesianIndices</span><span class="x">(</span><span class="n">ntuple</span><span class="x">(</span><span class="n">i</span><span class="o">-&gt;</span> <span class="mi">2</span><span class="o">:</span><span class="n">size</span><span class="x">(</span><span class="n">a</span><span class="x">)[</span><span class="n">i</span><span class="x">]</span><span class="o">-</span><span class="mi">1</span><span class="x">,</span><span class="n">ndims</span><span class="x">(</span><span class="n">a</span><span class="x">)))</span>

<span class="c"># serial loop macro</span>
<span class="k">macro</span><span class="nf"> loop</span><span class="x">(</span><span class="n">args</span><span class="o">...</span><span class="x">)</span>
    <span class="n">ex</span><span class="x">,</span><span class="n">_</span><span class="x">,</span><span class="n">itr</span> <span class="o">=</span> <span class="n">args</span>
    <span class="n">op</span><span class="x">,</span><span class="n">I</span><span class="x">,</span><span class="n">R</span> <span class="o">=</span> <span class="n">itr</span><span class="o">.</span><span class="n">args</span>
    <span class="nd">@assert</span> <span class="n">op</span> <span class="n">∈</span> <span class="x">(</span><span class="o">:</span><span class="x">(</span><span class="n">∈</span><span class="x">),</span><span class="o">:</span><span class="x">(</span><span class="k">in</span><span class="x">))</span>
    <span class="k">return</span> <span class="k">quote</span>
        <span class="k">for</span> <span class="o">$</span><span class="n">I</span> <span class="n">∈</span> <span class="o">$</span><span class="n">R</span>
            <span class="o">$</span><span class="n">ex</span>
        <span class="k">end</span>
    <span class="k">end</span> <span class="o">|&gt;</span> <span class="n">esc</span>
<span class="k">end</span>
<span class="c"># KA-adapted loop macro</span>
<span class="k">macro</span><span class="nf"> loopKA</span><span class="x">(</span><span class="n">args</span><span class="o">...</span><span class="x">)</span>
    <span class="n">ex</span><span class="x">,</span><span class="n">_</span><span class="x">,</span><span class="n">itr</span> <span class="o">=</span> <span class="n">args</span>
    <span class="n">_</span><span class="x">,</span><span class="n">I</span><span class="x">,</span><span class="n">R</span> <span class="o">=</span> <span class="n">itr</span><span class="o">.</span><span class="n">args</span><span class="x">;</span> <span class="n">sym</span> <span class="o">=</span> <span class="x">[]</span>
    <span class="n">grab!</span><span class="x">(</span><span class="n">sym</span><span class="x">,</span><span class="n">ex</span><span class="x">)</span>     <span class="c"># get arguments and replace composites in `ex`</span>
    <span class="n">setdiff!</span><span class="x">(</span><span class="n">sym</span><span class="x">,[</span><span class="n">I</span><span class="x">])</span> <span class="c"># don't want to pass I as an argument</span>
    <span class="nd">@gensym</span> <span class="n">kern</span>      <span class="c"># generate unique kernel function name</span>
    <span class="k">return</span> <span class="k">quote</span>
        <span class="nd">@kernel</span> <span class="k">function</span><span class="nf"> $kern</span><span class="x">(</span><span class="o">$</span><span class="x">(</span><span class="n">rep</span><span class="o">.</span><span class="x">(</span><span class="n">sym</span><span class="x">)</span><span class="o">...</span><span class="x">),</span><span class="nd">@Const</span><span class="x">(</span><span class="n">I0</span><span class="x">))</span> <span class="c"># replace composite arguments</span>
            <span class="o">$</span><span class="n">I</span> <span class="o">=</span> <span class="nd">@index</span><span class="x">(</span><span class="n">Global</span><span class="x">,</span><span class="n">Cartesian</span><span class="x">)</span>
            <span class="o">$</span><span class="n">I</span> <span class="o">+=</span> <span class="n">I0</span>
            <span class="o">$</span><span class="n">ex</span>
        <span class="k">end</span>
        <span class="o">$</span><span class="n">kern</span><span class="x">(</span><span class="n">get_backend</span><span class="x">(</span><span class="o">$</span><span class="x">(</span><span class="n">sym</span><span class="x">[</span><span class="mi">1</span><span class="x">])),</span><span class="mi">64</span><span class="x">)(</span><span class="o">$</span><span class="x">(</span><span class="n">sym</span><span class="o">...</span><span class="x">),</span><span class="o">$</span><span class="n">R</span><span class="x">[</span><span class="mi">1</span><span class="x">]</span><span class="o">-</span><span class="n">oneunit</span><span class="x">(</span><span class="o">$</span><span class="n">R</span><span class="x">[</span><span class="mi">1</span><span class="x">]),</span><span class="n">ndrange</span><span class="o">=</span><span class="n">size</span><span class="x">(</span><span class="o">$</span><span class="n">R</span><span class="x">))</span>
    <span class="k">end</span> <span class="o">|&gt;</span> <span class="n">esc</span>
<span class="k">end</span>
<span class="k">function</span><span class="nf"> grab!</span><span class="x">(</span><span class="n">sym</span><span class="x">,</span><span class="n">ex</span><span class="o">::</span><span class="kt">Expr</span><span class="x">)</span>
    <span class="n">ex</span><span class="o">.</span><span class="n">head</span> <span class="o">==</span> <span class="o">:.</span> <span class="o">&amp;&amp;</span> <span class="k">return</span> <span class="n">union!</span><span class="x">(</span><span class="n">sym</span><span class="x">,[</span><span class="n">ex</span><span class="x">])</span>      <span class="c"># grab composite name and return</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">ex</span><span class="o">.</span><span class="n">head</span><span class="o">==:</span><span class="x">(</span><span class="n">call</span><span class="x">)</span> <span class="o">?</span> <span class="mi">2</span> <span class="o">:</span> <span class="mi">1</span>              <span class="c"># don't grab function names</span>
    <span class="n">foreach</span><span class="x">(</span><span class="n">a</span><span class="o">-&gt;</span><span class="n">grab!</span><span class="x">(</span><span class="n">sym</span><span class="x">,</span><span class="n">a</span><span class="x">),</span><span class="n">ex</span><span class="o">.</span><span class="n">args</span><span class="x">[</span><span class="n">start</span><span class="o">:</span><span class="k">end</span><span class="x">])</span>   <span class="c"># recurse into args</span>
    <span class="n">ex</span><span class="o">.</span><span class="n">args</span><span class="x">[</span><span class="n">start</span><span class="o">:</span><span class="k">end</span><span class="x">]</span> <span class="o">=</span> <span class="n">rep</span><span class="o">.</span><span class="x">(</span><span class="n">ex</span><span class="o">.</span><span class="n">args</span><span class="x">[</span><span class="n">start</span><span class="o">:</span><span class="k">end</span><span class="x">])</span> <span class="c"># replace composites in args</span>
<span class="k">end</span>
<span class="n">grab!</span><span class="x">(</span><span class="n">sym</span><span class="x">,</span><span class="n">ex</span><span class="o">::</span><span class="kt">Symbol</span><span class="x">)</span> <span class="o">=</span> <span class="n">union!</span><span class="x">(</span><span class="n">sym</span><span class="x">,[</span><span class="n">ex</span><span class="x">])</span>        <span class="c"># grab symbol name</span>
<span class="n">grab!</span><span class="x">(</span><span class="n">sym</span><span class="x">,</span><span class="n">ex</span><span class="x">)</span> <span class="o">=</span> <span class="nb">nothing</span>
<span class="n">rep</span><span class="x">(</span><span class="n">ex</span><span class="x">)</span> <span class="o">=</span> <span class="n">ex</span>
<span class="n">rep</span><span class="x">(</span><span class="n">ex</span><span class="o">::</span><span class="kt">Expr</span><span class="x">)</span> <span class="o">=</span> <span class="n">ex</span><span class="o">.</span><span class="n">head</span> <span class="o">==</span> <span class="o">:.</span> <span class="o">?</span> <span class="kt">Symbol</span><span class="x">(</span><span class="n">ex</span><span class="o">.</span><span class="n">args</span><span class="x">[</span><span class="mi">2</span><span class="x">]</span><span class="o">.</span><span class="n">value</span><span class="x">)</span> <span class="o">:</span> <span class="n">ex</span>

<span class="k">function</span><span class="nf"> divergence!</span><span class="x">(</span><span class="n">σ</span><span class="x">,</span> <span class="n">u</span><span class="x">)</span>
    <span class="k">for</span> <span class="n">d</span> <span class="n">∈</span> <span class="mi">1</span><span class="o">:</span><span class="n">ndims</span><span class="x">(</span><span class="n">σ</span><span class="x">)</span>
        <span class="nd">@loop</span> <span class="n">σ</span><span class="x">[</span><span class="n">I</span><span class="x">]</span> <span class="o">+=</span> <span class="n">∂</span><span class="x">(</span><span class="n">d</span><span class="x">,</span> <span class="n">I</span><span class="x">,</span> <span class="n">u</span><span class="x">)</span> <span class="n">over</span> <span class="n">I</span> <span class="n">∈</span> <span class="n">inside</span><span class="x">(</span><span class="n">σ</span><span class="x">)</span>
    <span class="k">end</span>
<span class="k">end</span>
<span class="k">function</span><span class="nf"> divergenceKA!</span><span class="x">(</span><span class="n">σ</span><span class="x">,</span> <span class="n">u</span><span class="x">)</span>
    <span class="k">for</span> <span class="n">d</span> <span class="n">∈</span> <span class="mi">1</span><span class="o">:</span><span class="n">ndims</span><span class="x">(</span><span class="n">σ</span><span class="x">)</span>
        <span class="nd">@loopKA</span> <span class="n">σ</span><span class="x">[</span><span class="n">I</span><span class="x">]</span> <span class="o">+=</span> <span class="n">∂</span><span class="x">(</span><span class="n">d</span><span class="x">,</span> <span class="n">I</span><span class="x">,</span> <span class="n">u</span><span class="x">)</span> <span class="n">over</span> <span class="n">I</span> <span class="n">∈</span> <span class="n">inside</span><span class="x">(</span><span class="n">σ</span><span class="x">)</span>
    <span class="k">end</span>
<span class="k">end</span>

<span class="n">N</span> <span class="o">=</span> <span class="x">(</span><span class="mi">2</span><span class="o">^</span><span class="mi">8</span><span class="x">,</span> <span class="mi">2</span><span class="o">^</span><span class="mi">8</span><span class="x">,</span> <span class="mi">2</span><span class="o">^</span><span class="mi">8</span><span class="x">)</span>
<span class="c"># CPU serial arrays</span>
<span class="n">σ_serial</span> <span class="o">=</span> <span class="n">zeros</span><span class="x">(</span><span class="n">N</span><span class="x">)</span>
<span class="n">u_serial</span> <span class="o">=</span> <span class="n">rand</span><span class="x">(</span><span class="n">N</span><span class="o">...</span><span class="x">,</span> <span class="n">length</span><span class="x">(</span><span class="n">N</span><span class="x">))</span>
<span class="c"># CPU multi-threading arrays</span>
<span class="n">σ_CPU</span> <span class="o">=</span> <span class="n">zeros</span><span class="x">(</span><span class="n">N</span><span class="x">)</span>
<span class="n">u_CPU</span> <span class="o">=</span> <span class="n">copy</span><span class="x">(</span><span class="n">u_serial</span><span class="x">)</span>
<span class="c"># GPU arrays</span>
<span class="n">σ_GPU</span> <span class="o">=</span> <span class="n">zeros</span><span class="x">(</span><span class="n">N</span><span class="x">)</span> <span class="o">|&gt;</span> <span class="n">CuArray</span>
<span class="n">u_GPU</span> <span class="o">=</span> <span class="n">copy</span><span class="x">(</span><span class="n">u_serial</span><span class="x">)</span> <span class="o">|&gt;</span> <span class="n">CuArray</span>

<span class="c"># Benchmark warmup (force compilation) and validation</span>
<span class="n">divergence!</span><span class="x">(</span><span class="n">σ_serial</span><span class="x">,</span> <span class="n">u_serial</span><span class="x">)</span>
<span class="n">divergenceKA!</span><span class="x">(</span><span class="n">σ_CPU</span><span class="x">,</span> <span class="n">u_CPU</span><span class="x">)</span>
<span class="n">divergenceKA!</span><span class="x">(</span><span class="n">σ_GPU</span><span class="x">,</span> <span class="n">u_GPU</span><span class="x">)</span>
<span class="nd">@assert</span> <span class="n">σ_serial</span> <span class="n">≈</span> <span class="n">σ_CPU</span> <span class="n">≈</span> <span class="n">σ_GPU</span> <span class="o">|&gt;</span> <span class="kt">Array</span>

<span class="c"># Create and run benchmarks</span>
<span class="n">suite</span> <span class="o">=</span> <span class="n">BenchmarkGroup</span><span class="x">()</span>
<span class="n">suite</span><span class="x">[</span><span class="s">"serial"</span><span class="x">]</span> <span class="o">=</span> <span class="nd">@benchmarkable</span> <span class="n">divergence!</span><span class="x">(</span><span class="o">$</span><span class="n">σ_serial</span><span class="x">,</span> <span class="o">$</span><span class="n">u_serial</span><span class="x">)</span>
<span class="n">suite</span><span class="x">[</span><span class="s">"CPU"</span><span class="x">]</span> <span class="o">=</span> <span class="nd">@benchmarkable</span> <span class="k">begin</span>
    <span class="n">divergenceKA!</span><span class="x">(</span><span class="o">$</span><span class="n">σ_CPU</span><span class="x">,</span> <span class="o">$</span><span class="n">u_CPU</span><span class="x">)</span>
    <span class="n">synchronize</span><span class="x">(</span><span class="n">get_backend</span><span class="x">(</span><span class="o">$</span><span class="n">σ_CPU</span><span class="x">))</span>
<span class="k">end</span>
<span class="n">suite</span><span class="x">[</span><span class="s">"GPU"</span><span class="x">]</span> <span class="o">=</span> <span class="nd">@benchmarkable</span> <span class="k">begin</span>
    <span class="n">divergenceKA!</span><span class="x">(</span><span class="o">$</span><span class="n">σ_GPU</span><span class="x">,</span> <span class="o">$</span><span class="n">u_GPU</span><span class="x">)</span>
    <span class="n">synchronize</span><span class="x">(</span><span class="n">get_backend</span><span class="x">(</span><span class="o">$</span><span class="n">σ_GPU</span><span class="x">))</span>
<span class="k">end</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">run</span><span class="x">(</span><span class="n">suite</span><span class="x">,</span> <span class="n">verbose</span><span class="o">=</span><span class="nb">true</span><span class="x">)</span>
</code></pre></div></div>

<p>In this benchmark we have used a 3D array <code class="language-plaintext highlighter-rouge">σ</code> (scalar field) instead of the 2D array used before, hence demonstrating the n-dimensional capabilities of the current methodology.
For <code class="language-plaintext highlighter-rouge">N=(2^8,2^8,2^8)</code>, the following benchmark results are achieved on a 6-core laptop equipped with an NVIDIA GeForce GTX 1650 Ti GPU card</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"CPU" =&gt; Trial(52.651 ms)
"GPU" =&gt; Trial(7.589 ms)
"serial" =&gt; Trial(234.347 ms)
</code></pre></div></div>
<p>The GPU executions yields a <strong>30x</strong> speed-up compared to the serial execution and 7x compared to the multi-threaded CPU execution. The multi-threaded CPU execution yields 4.5x speed-up compared to the serial execution (ideally should be 6x in the 6-core machine).
As a final note on this section, see that <code class="language-plaintext highlighter-rouge">synchronize</code> is used when running the KA benchmarks. If not used, we would only be measuring the time that it takes to launch a kernel but not to actually run it.</p>

<h3 id="challenges">Challenges</h3>

<p>Porting the whole solver to GPU has been mostly a learning exercise.
With no previous experience on software development for GPUs, KA smoothens the learning curve, so it is a great way to get started.
Of course, a lot of stuff does not just work out of the box, and we have faced some challenges while doing the port. Here are some of them.</p>

<h4 id="offset-indices-in-ka-kernels">Offset indices in KA kernels</h4>
<p>Offset indices are important for boundary-value problems where arrays may contain both the solution and the boundary conditions of a problem.
In the stencil-based finite-volume and finite-difference methods, the boundary elements are only accessed to compute the stencil, but not directly modified when looping through the solution elements of an array.
It is in this scenario where offset indices are important, for example.
KA <code class="language-plaintext highlighter-rouge">@index</code> macro only provides natural indices in Julia (starting at 1), and this minor missing feature initially derailed us into using <a href="https://github.com/JuliaArrays/OffsetArrays.jl">OffsetArrays.jl</a>.
Of course this added complexity to the code, and we even observed degraded performance in some kernels.
Some time after this (more than we would like to admit), the idea of manually passing the offset index into the KA kernel took shape and quickly yield a much cleaner solution.
Thankfully, this feature will be natively supported in KA in the future (see <a href="https://github.com/JuliaGPU/KernelAbstractions.jl/issues/384">KA issue #384</a>).</p>

<h4 id="to-inline-functions-can-be-important-in-gpu-kernels">To inline functions can be important in GPU kernels</h4>
<p>In KA, GPU kernels are of course more sensitive than CPU kernels when it comes to functions that may be called within.
We have observed this sensitivity both at compilation time and at runtime.
For example, the <code class="language-plaintext highlighter-rouge">δ</code> function was originally implemented with multiple dispatch as</p>
<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@inline</span> <span class="n">δ</span><span class="x">(</span><span class="n">i</span><span class="x">,</span><span class="n">N</span><span class="o">::</span><span class="kt">Int</span><span class="x">)</span> <span class="o">=</span> <span class="kt">CartesianIndex</span><span class="x">(</span><span class="n">ntuple</span><span class="x">(</span><span class="n">j</span> <span class="o">-&gt;</span> <span class="n">j</span><span class="o">==</span><span class="n">i</span> <span class="o">?</span> <span class="mi">1</span> <span class="o">:</span> <span class="mi">0</span><span class="x">,</span> <span class="n">N</span><span class="x">))</span>
<span class="n">δ</span><span class="x">(</span><span class="n">d</span><span class="x">,</span><span class="n">I</span><span class="o">::</span><span class="kt">CartesianIndex</span><span class="x">{</span><span class="n">N</span><span class="x">})</span> <span class="k">where</span> <span class="x">{</span><span class="n">N</span><span class="x">}</span> <span class="o">=</span> <span class="n">δ</span><span class="x">(</span><span class="n">d</span><span class="x">,</span> <span class="n">N</span><span class="x">)</span>
</code></pre></div></div>
<p>The main problem here is that this implementation is type-unstable, and without <code class="language-plaintext highlighter-rouge">@inline</code> the GPU kernel was complaining about a dynamic function (see <a href="https://github.com/JuliaGPU/KernelAbstractions.jl/issues/392">KA issue #392</a>).
Another inline-related problem can be observed with the derivative function <code class="language-plaintext highlighter-rouge">∂</code>.
When removing the <code class="language-plaintext highlighter-rouge">@inline</code> macro from its definition, the GPU performance decays significantly, and the GPU benchmark gets even with the CPU one.
This demonstrates that the compiler can do performant tricks when the information on the required instructions is not nested on external functions to the kernel.</p>

<h4 id="popular-functions-may-not-work-within-kernels">Popular functions may not work within kernels</h4>
<p>Often we use functions such as the <code class="language-plaintext highlighter-rouge">norm2</code> from LinearAlgebra.jl to compute the norm of an array.
A surprise is that some of these do not work inside a kernel since the GPU compiler may not be equipped to do so. Hence, these need to be manually written in a suitable form.
In this case, we use <code class="language-plaintext highlighter-rouge">norm2(x) = √sum(abs2,x)</code>.
Another example is the <code class="language-plaintext highlighter-rouge">sum</code> function using generator syntax such as</p>
<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@kernel</span> <span class="k">function</span><span class="nf"> _divergence</span><span class="x">(</span><span class="n">σ</span><span class="x">,</span> <span class="n">u</span><span class="x">)</span>
    <span class="n">I</span> <span class="o">=</span> <span class="nd">@index</span><span class="x">(</span><span class="n">Global</span><span class="x">,</span> <span class="n">Cartesian</span><span class="x">)</span>
    <span class="n">σ</span><span class="x">[</span><span class="n">I</span><span class="x">]</span> <span class="o">=</span> <span class="n">sum</span><span class="x">(</span><span class="n">u</span><span class="x">[</span><span class="n">I</span><span class="o">+</span><span class="n">δ</span><span class="x">(</span><span class="n">d</span><span class="x">),</span><span class="n">d</span><span class="x">]</span><span class="o">-</span><span class="n">u</span><span class="x">[</span><span class="n">I</span><span class="x">,</span><span class="n">d</span><span class="x">]</span> <span class="k">for</span> <span class="n">d</span> <span class="n">∈</span> <span class="mi">1</span><span class="o">:</span><span class="n">ndims</span><span class="x">(</span><span class="n">σ</span><span class="x">))</span>
<span class="k">end</span>
</code></pre></div></div>
<p>which errors during compilation for a GPU kernel.
Here a solution can be to use a different form of <code class="language-plaintext highlighter-rouge">sum</code></p>
<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@kernel</span> <span class="k">function</span><span class="nf"> _divergence</span><span class="x">(</span><span class="n">σ</span><span class="x">,</span> <span class="n">u</span><span class="x">)</span>
    <span class="n">I</span> <span class="o">=</span> <span class="nd">@index</span><span class="x">(</span><span class="n">Global</span><span class="x">,</span> <span class="n">Cartesian</span><span class="x">)</span>
    <span class="n">σ</span><span class="x">[</span><span class="n">I</span><span class="x">]</span> <span class="o">=</span> <span class="n">sum</span><span class="x">(</span><span class="n">j</span> <span class="o">-&gt;</span> <span class="n">u</span><span class="x">[</span><span class="n">I</span><span class="o">+</span><span class="n">δ</span><span class="x">(</span><span class="n">j</span><span class="x">),</span><span class="n">j</span><span class="x">]</span><span class="o">-</span><span class="n">u</span><span class="x">[</span><span class="n">I</span><span class="x">,</span><span class="n">j</span><span class="x">],</span> <span class="mi">1</span><span class="o">:</span><span class="n">ndims</span><span class="x">(</span><span class="n">σ</span><span class="x">),</span> <span class="n">init</span><span class="o">=</span><span class="n">zero</span><span class="x">(</span><span class="n">eltype</span><span class="x">(</span><span class="n">σ</span><span class="x">)))</span>
<span class="k">end</span>
</code></pre></div></div>
<p>even though we have observed reduced performance in the latter version (more information in <a href="https://discourse.julialang.org/t/gpu-sum-closure-throwing-an-error/96658">Discourse post #96658</a>).
There are efforts in KA directed towards providing a reduction interface for kernels (see <a href="https://github.com/JuliaGPU/KernelAbstractions.jl/issues/234">KA issue #234</a>).</p>

<h4 id="limitations-of-the-automatic-kernel-generation-on-loops">Limitations of the automatic kernel generation on loops</h4>
<p>While the <code class="language-plaintext highlighter-rouge">@loop</code> macro that generates KA kernels is fairly general, it also has some limitations.
For example, it may have been noticed that we have not nested the loop over the dimensions <code class="language-plaintext highlighter-rouge">d ∈ 1:ndims(σ)</code> in the kernel.
The reason behind this is that even if turning</p>
<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">d</span> <span class="n">∈</span> <span class="mi">1</span><span class="o">:</span><span class="n">ndims</span><span class="x">(</span><span class="n">σ</span><span class="x">)</span>
    <span class="nd">@loop</span> <span class="n">σ</span><span class="x">[</span><span class="n">I</span><span class="x">]</span> <span class="o">+=</span> <span class="n">∂</span><span class="x">(</span><span class="n">d</span><span class="x">,</span> <span class="n">I</span><span class="x">,</span> <span class="n">u</span><span class="x">)</span> <span class="n">over</span> <span class="n">I</span> <span class="n">∈</span> <span class="n">inside</span><span class="x">(</span><span class="n">σ</span><span class="x">)</span>
<span class="k">end</span>
</code></pre></div></div>
<p>into</p>
<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@loop</span> <span class="n">σ</span><span class="x">[</span><span class="n">I</span><span class="x">]</span> <span class="o">=</span> <span class="n">sum</span><span class="x">(</span><span class="n">d</span><span class="o">-&gt;</span><span class="n">∂</span><span class="x">(</span><span class="n">d</span><span class="x">,</span> <span class="n">I</span><span class="x">,</span> <span class="n">u</span><span class="x">),</span> <span class="mi">1</span><span class="o">:</span><span class="n">ndims</span><span class="x">(</span><span class="n">σ</span><span class="x">))</span> <span class="n">over</span> <span class="n">I</span> <span class="n">∈</span> <span class="n">inside</span><span class="x">(</span><span class="n">σ</span><span class="x">)</span>
</code></pre></div></div>
<p>would reduce the number of kernel evaluations, the limitation of the <code class="language-plaintext highlighter-rouge">sum</code> function mentioned before makes this approach not as performant as writing a kernel for each dimension.
Also related to this issue is the fact that passing more than one expression per kernel would reduce the overall number of kernel evaluations, but gluing expressions together can be not straight-forward with the current implementation of <code class="language-plaintext highlighter-rouge">@loop</code>.</p>

<h4 id="care-for-race-conditions">Care for race conditions!</h4>
<p>When moving from serial to parallel computations, race conditions are a recurring issue.
For WaterLily, this issue popped up for the linear solver used in the pressure Poisson equation.
Prior to the port, WaterLily relied on Successive Over Relaxation (SOR) method (a Gauss-Seidel-type solver) which uses (ordered) backsubstitution, hence not suitable for parallel executions.
The solution here was just to switch to a better suited solver such as the Conjugate-Gradient method.</p>

<h3 id="acknowledgements">Acknowledgements</h3>

<p>Special thanks to <a href="https://vchuravy.dev/">Valentin Churavy</a> for creating <a href="https://github.com/JuliaGPU/KernelAbstractions.jl">KernelAbstractions.jl</a> and revising this article. And, of course, <a href="https://weymouth.github.io/">Gabriel D. Weymouth</a> for creating <a href="https://github.com/weymouth/WaterLily.jl">WaterLily.jl</a> and for helping in the revising of this article too! :)</p>]]></content><author><name>Bernat Font</name><email>bernatfontgarcia@gmail.com</email></author><category term="julia" /><category term="gpu" /><category term="waterlily" /><summary type="html"><![CDATA[WaterLily.jl is a simple and fast fluid simulator written in pure Julia. It solves the unsteady incompressible 2D or 3D Navier-Stokes equations on a Cartesian grid. The pressure Poisson equation is solved with a geometric multigrid method. Solid boundaries are modelled using the Boundary Data Immersion Method. v1.0 has ported the solver from a serial CPU execution to a backend-agnostic execution including multi-threaded CPU and GPU from different vendors (NVIDIA and AMD) thanks to KernelAbstractions.jl (KA). We have also recently published an extended abstract preprint with benchmarking details regarding this port (see arXiv). In this post, we will review our approach to port the code together with the challenges we have faced. Introducing KernelAbstractions.jl The main ingredient of this port is the @kernel macro from KA. This macro takes a function definition and converts it into a kernel specialised for a given backend. KA can work with CUDA, ROCm, oneAPI, and Metal backends. As example, consider the divergence operator for a 2D vector field $\vec{u}=(u, v)$. In the finite-volume method (FVM) and using a Cartesian (uniform) grid with unit cells, this is defined as \[\begin{align} \sigma=\unicode{x2230}(\nabla\cdot\vec{u})\,\mathrm{d}V = \unicode{x222F}\vec{u}\cdot\hat{n}\,\mathrm{d}S\rightarrow \sigma_{i,j} = (u_{i+1,j} - u_{i,j}) + (v_{i,j+1} - v_{i,j}), \end{align}\] where $i$ and $j$ are the indices of the discretised staggered grid: In WaterLily, we define loops based on the CartesianIndex such that I=(i,j,...), thus writing an n-dimensional solver in a very straight-forward way. With this, to compute the divergence of a 2D vector field we can use δ(d,::CartesianIndex{D}) where {D} = CartesianIndex(ntuple(j -&gt; j==d ? 1 : 0, D)) @inline ∂(a,I::CartesianIndex{D},u::AbstractArray{T,n}) where {D,T,n} = u[I+δ(a,I),a] - u[I,a] inside(a) = CartesianIndices(ntuple(i-&gt; 2:size(a)[i]-1, ndims(a))) N = (10, 10) # domain size σ = zeros(N) # scalar field u = rand(N..., length(N)) # 2D vector field with ghost cells for d ∈ 1:ndims(σ), I ∈ inside(σ) σ[I] += ∂(d, I, u) end where a loop for each dimension d and each Cartesian index I is used. The function δ provides a Cartesian step in the direction d, for example δ(1, 2) returns CartesianIndex(1, 0) and δ(2, 3) returns CartesianIndex(0, 1, 0). This is used in the derivative function ∂ to implement the divergence equation as described above. inside(σ) provides the CartesianIndices of σ excluding the ghost elements, ie. a range of Cartesian indices starting at (2, 2) and ending at (9, 9) when size(σ) == (10, 10). Note that the divergence operation is independent for each I, and this is great because it means we can parallelize it! This is where KA comes into place. To be able to generate the divergence operator using KA, we need to write the divergence kernel which is just the divergence operator written in KA style. We define the divergence kernel _divergence! and a wrapper divergence! as follows using KernelAbstractions: get_backend, @index, @kernel @kernel function _divergence!(σ, u, @Const(I0)) I = @index(Global, Cartesian) I += I0 σ_sum = zero(eltype(σ)) for d ∈ 1:ndims(σ) σ_sum += ∂(d, I, u) end σ[I] = σ_sum end function divergence!(σ, u) R = inside(σ) _divergence!(get_backend(σ), 64)(σ, u, R[1]-oneunit(R[1]), ndrange=size(R)) end Note that in the _divergence! kernel we operate again using Cartesian indices by calling @index(Global, Cartesian) from the KA @index macro. The range of Cartesian indices is given by the ndrange argument in the wrapper function where we pass the inside(σ) Cartesian indices range, and the backend is inferred with the get_backend method. Also note that we pass an additional (constant) argument I0 which provides the offset index to apply to the indices given by @index naturally starting on (1,1,...). Using a workgroup size of 64 (size of the group of threads acting in parallel, see terminology), KA will parallelize over I by multi-threading in CPU or GPU. In this regard, we just need to change the array type of σ and u from Array (CPU backend) to CuArray (NVIDIA GPU backend) or ROCArray (AMD GPU backend), and KA will specialise the kernel for the desired backend using CUDA: CuArray N = (10, 10) σ = zeros(N) |&gt; CuArray u = rand(N..., length(N)) |&gt; CuArray divergence!(σ, u) Automatic loop and kernel generation As a stencil-based CFD solver, WaterLily heavily uses for loops to iterate over the n-dimensional arrays. To automate the generation of such loops, the macro @loop is defined macro loop(args...) ex,_,itr = args op,I,R = itr.args @assert op ∈ (:(∈),:(in)) return quote for $I ∈ $R $ex end end |&gt; esc end This macro takes an expression such as @loop &lt;expr&gt; over I ∈ R where R is a CartesianIndices range, and produces the loop for I ∈ R &lt;expr&gt; end. For example, the serial divergence operator could now be simply defined using for d ∈ 1:ndims(σ) @loop σ[I] += ∂(d, I, u) over I ∈ inside(σ) end which generates the I ∈ inside(σ) loop automatically. Even though this could be seen as small improvement (if any), the nice thing about writing loops using this approach is that the computationally-demanding part of the code can be abstracted out of the main workflow. For example, it is easy to add performance macros such as @inbounds and/or @fastmath to each loop by changing the quote block in the @loop macro macro loop(args...) ex,_,itr = args op,I,R = itr.args @assert op ∈ (:(∈),:(in)) return quote @inbounds for $I ∈ $R @fastmath $ex end end |&gt; esc end And, even nicer, we can also use this approach to automatically generate KA kernels for every loop in the code! To do so, we modify @loop to generate the KA kernel using @kernel and the wrapper function that sets the backend and the workgroup size macro loop(args...) ex,_,itr = args _,I,R = itr.args; sym = [] grab!(sym,ex) # get arguments and replace composites in `ex` setdiff!(sym,[I]) # don't want to pass I as an argument @gensym kern # generate unique kernel function name return quote @kernel function $kern($(rep.(sym)...),@Const(I0)) # replace composite arguments $I = @index(Global,Cartesian) $I += I0 $ex end $kern(get_backend($(sym[1])),64)($(sym...),$R[1]-oneunit($R[1]),ndrange=size($R)) end |&gt; esc end function grab!(sym,ex::Expr) ex.head == :. &amp;&amp; return union!(sym,[ex]) # grab composite name and return start = ex.head==:(call) ? 2 : 1 # don't grab function names foreach(a-&gt;grab!(sym,a),ex.args[start:end]) # recurse into args ex.args[start:end] = rep.(ex.args[start:end]) # replace composites in args end grab!(sym,ex::Symbol) = union!(sym,[ex]) # grab symbol name grab!(sym,ex) = nothing rep(ex) = ex rep(ex::Expr) = ex.head == :. ? Symbol(ex.args[2].value) : ex The helper functions grab! and rep allow to extract the arguments required by the expression ex and the Cartesian index range that will be passed to the kernel. The code generated by @loop and @kernel can be explored using @macroexpand. For example, for d=1 @macroexpand @loop σ[I] += ∂(1, I, u) over I ∈ inside(σ) we can observe that the code for both CPU and GPU kernels is produced: Generated code @macroexpand @loopKA σ[I] += ∂(1, I, u) over I ∈ inside(σ) quote begin function var"cpu_##kern#339"(__ctx__, σ, u, I0; ) let I0 = (KernelAbstractions.constify)(I0) $(Expr(:aliasscope)) begin var"##N#341" = length((KernelAbstractions.__workitems_iterspace)(__ctx__)) begin for var"##I#340" = (KernelAbstractions.__workitems_iterspace)(__ctx__) (KernelAbstractions.__validindex)(__ctx__, var"##I#340") || continue I = KernelAbstractions.__index_Global_Cartesian(__ctx__, var"##I#340") begin I += I0 σ[I] += ∂(1, I, u) end end end end $(Expr(:popaliasscope)) return nothing end end function var"gpu_##kern#339"(__ctx__, σ, u, I0; ) let I0 = (KernelAbstractions.constify)(I0) if (KernelAbstractions.__validindex)(__ctx__) begin I = KernelAbstractions.__index_Global_Cartesian(__ctx__) I += I0 σ[I] += ∂(1, I, u) end end return nothing end end begin if !($(Expr(:isdefined, Symbol("##kern#339")))) begin $(Expr(:meta, :doc)) var"##kern#339"(dev) = begin var"##kern#339"(dev, (KernelAbstractions.NDIteration.DynamicSize)(), (KernelAbstractions.NDIteration.DynamicSize)()) end end var"##kern#339"(dev, size) = begin var"##kern#339"(dev, (KernelAbstractions.NDIteration.StaticSize)(size), (KernelAbstractions.NDIteration.DynamicSize)()) end var"##kern#339"(dev, size, range) = begin var"##kern#339"(dev, (KernelAbstractions.NDIteration.StaticSize)(size), (KernelAbstractions.NDIteration.StaticSize)(range)) end function var"##kern#339"(dev::Dev, sz::S, range::NDRange) where {Dev, S &lt;: KernelAbstractions.NDIteration._Size, NDRange &lt;: KernelAbstractions.NDIteration._Size} if (KernelAbstractions.isgpu)(dev) return (KernelAbstractions.construct)(dev, sz, range, var"gpu_##kern#339") else return (KernelAbstractions.construct)(dev, sz, range, var"cpu_##kern#339") end end end end end (var"##kern#339"(get_backend(σ), 64))(σ, u, (inside(σ))[1] - oneunit((inside(σ))[1]), ndrange = size(inside(σ))) end The best feature we achieve when modifying @loop to produce KA kernels is that the divergence operator remains the same as before using KA for d ∈ 1:ndims(σ) @loop σ[I] += ∂(d, I, u) over I ∈ inside(σ) end This exact approach is what has allowed WaterLily to have the same LOC as before using KA, just around 800! Benchmarking Now that we have all the items in place, we can benchmark the speedup achieved by KA compared to the serial execution using BenchmarkTools.jl. Let’s now gather all the code we have used and create a small benchmarking MWE (see below or download it here). In this code showcase, we will refer to the serial CPU execution as “serial”, the multi-threaded CPU execution as “CPU”, and the GPU execution as “GPU”: using KernelAbstractions: get_backend, synchronize, @index, @kernel, @groupsize using CUDA: CuArray using BenchmarkTools δ(d,::CartesianIndex{D}) where {D} = CartesianIndex(ntuple(j -&gt; j==d ? 1 : 0, D)) @inline ∂(a,I::CartesianIndex{D},u::AbstractArray{T,n}) where {D,T,n} = u[I+δ(a,I),a]-u[I,a] inside(a) = CartesianIndices(ntuple(i-&gt; 2:size(a)[i]-1,ndims(a))) # serial loop macro macro loop(args...) ex,_,itr = args op,I,R = itr.args @assert op ∈ (:(∈),:(in)) return quote for $I ∈ $R $ex end end |&gt; esc end # KA-adapted loop macro macro loopKA(args...) ex,_,itr = args _,I,R = itr.args; sym = [] grab!(sym,ex) # get arguments and replace composites in `ex` setdiff!(sym,[I]) # don't want to pass I as an argument @gensym kern # generate unique kernel function name return quote @kernel function $kern($(rep.(sym)...),@Const(I0)) # replace composite arguments $I = @index(Global,Cartesian) $I += I0 $ex end $kern(get_backend($(sym[1])),64)($(sym...),$R[1]-oneunit($R[1]),ndrange=size($R)) end |&gt; esc end function grab!(sym,ex::Expr) ex.head == :. &amp;&amp; return union!(sym,[ex]) # grab composite name and return start = ex.head==:(call) ? 2 : 1 # don't grab function names foreach(a-&gt;grab!(sym,a),ex.args[start:end]) # recurse into args ex.args[start:end] = rep.(ex.args[start:end]) # replace composites in args end grab!(sym,ex::Symbol) = union!(sym,[ex]) # grab symbol name grab!(sym,ex) = nothing rep(ex) = ex rep(ex::Expr) = ex.head == :. ? Symbol(ex.args[2].value) : ex function divergence!(σ, u) for d ∈ 1:ndims(σ) @loop σ[I] += ∂(d, I, u) over I ∈ inside(σ) end end function divergenceKA!(σ, u) for d ∈ 1:ndims(σ) @loopKA σ[I] += ∂(d, I, u) over I ∈ inside(σ) end end N = (2^8, 2^8, 2^8) # CPU serial arrays σ_serial = zeros(N) u_serial = rand(N..., length(N)) # CPU multi-threading arrays σ_CPU = zeros(N) u_CPU = copy(u_serial) # GPU arrays σ_GPU = zeros(N) |&gt; CuArray u_GPU = copy(u_serial) |&gt; CuArray # Benchmark warmup (force compilation) and validation divergence!(σ_serial, u_serial) divergenceKA!(σ_CPU, u_CPU) divergenceKA!(σ_GPU, u_GPU) @assert σ_serial ≈ σ_CPU ≈ σ_GPU |&gt; Array # Create and run benchmarks suite = BenchmarkGroup() suite["serial"] = @benchmarkable divergence!($σ_serial, $u_serial) suite["CPU"] = @benchmarkable begin divergenceKA!($σ_CPU, $u_CPU) synchronize(get_backend($σ_CPU)) end suite["GPU"] = @benchmarkable begin divergenceKA!($σ_GPU, $u_GPU) synchronize(get_backend($σ_GPU)) end results = run(suite, verbose=true) In this benchmark we have used a 3D array σ (scalar field) instead of the 2D array used before, hence demonstrating the n-dimensional capabilities of the current methodology. For N=(2^8,2^8,2^8), the following benchmark results are achieved on a 6-core laptop equipped with an NVIDIA GeForce GTX 1650 Ti GPU card "CPU" =&gt; Trial(52.651 ms) "GPU" =&gt; Trial(7.589 ms) "serial" =&gt; Trial(234.347 ms) The GPU executions yields a 30x speed-up compared to the serial execution and 7x compared to the multi-threaded CPU execution. The multi-threaded CPU execution yields 4.5x speed-up compared to the serial execution (ideally should be 6x in the 6-core machine). As a final note on this section, see that synchronize is used when running the KA benchmarks. If not used, we would only be measuring the time that it takes to launch a kernel but not to actually run it. Challenges Porting the whole solver to GPU has been mostly a learning exercise. With no previous experience on software development for GPUs, KA smoothens the learning curve, so it is a great way to get started. Of course, a lot of stuff does not just work out of the box, and we have faced some challenges while doing the port. Here are some of them. Offset indices in KA kernels Offset indices are important for boundary-value problems where arrays may contain both the solution and the boundary conditions of a problem. In the stencil-based finite-volume and finite-difference methods, the boundary elements are only accessed to compute the stencil, but not directly modified when looping through the solution elements of an array. It is in this scenario where offset indices are important, for example. KA @index macro only provides natural indices in Julia (starting at 1), and this minor missing feature initially derailed us into using OffsetArrays.jl. Of course this added complexity to the code, and we even observed degraded performance in some kernels. Some time after this (more than we would like to admit), the idea of manually passing the offset index into the KA kernel took shape and quickly yield a much cleaner solution. Thankfully, this feature will be natively supported in KA in the future (see KA issue #384). To inline functions can be important in GPU kernels In KA, GPU kernels are of course more sensitive than CPU kernels when it comes to functions that may be called within. We have observed this sensitivity both at compilation time and at runtime. For example, the δ function was originally implemented with multiple dispatch as @inline δ(i,N::Int) = CartesianIndex(ntuple(j -&gt; j==i ? 1 : 0, N)) δ(d,I::CartesianIndex{N}) where {N} = δ(d, N) The main problem here is that this implementation is type-unstable, and without @inline the GPU kernel was complaining about a dynamic function (see KA issue #392). Another inline-related problem can be observed with the derivative function ∂. When removing the @inline macro from its definition, the GPU performance decays significantly, and the GPU benchmark gets even with the CPU one. This demonstrates that the compiler can do performant tricks when the information on the required instructions is not nested on external functions to the kernel. Popular functions may not work within kernels Often we use functions such as the norm2 from LinearAlgebra.jl to compute the norm of an array. A surprise is that some of these do not work inside a kernel since the GPU compiler may not be equipped to do so. Hence, these need to be manually written in a suitable form. In this case, we use norm2(x) = √sum(abs2,x). Another example is the sum function using generator syntax such as @kernel function _divergence(σ, u) I = @index(Global, Cartesian) σ[I] = sum(u[I+δ(d),d]-u[I,d] for d ∈ 1:ndims(σ)) end which errors during compilation for a GPU kernel. Here a solution can be to use a different form of sum @kernel function _divergence(σ, u) I = @index(Global, Cartesian) σ[I] = sum(j -&gt; u[I+δ(j),j]-u[I,j], 1:ndims(σ), init=zero(eltype(σ))) end even though we have observed reduced performance in the latter version (more information in Discourse post #96658). There are efforts in KA directed towards providing a reduction interface for kernels (see KA issue #234). Limitations of the automatic kernel generation on loops While the @loop macro that generates KA kernels is fairly general, it also has some limitations. For example, it may have been noticed that we have not nested the loop over the dimensions d ∈ 1:ndims(σ) in the kernel. The reason behind this is that even if turning for d ∈ 1:ndims(σ) @loop σ[I] += ∂(d, I, u) over I ∈ inside(σ) end into @loop σ[I] = sum(d-&gt;∂(d, I, u), 1:ndims(σ)) over I ∈ inside(σ) would reduce the number of kernel evaluations, the limitation of the sum function mentioned before makes this approach not as performant as writing a kernel for each dimension. Also related to this issue is the fact that passing more than one expression per kernel would reduce the overall number of kernel evaluations, but gluing expressions together can be not straight-forward with the current implementation of @loop. Care for race conditions! When moving from serial to parallel computations, race conditions are a recurring issue. For WaterLily, this issue popped up for the linear solver used in the pressure Poisson equation. Prior to the port, WaterLily relied on Successive Over Relaxation (SOR) method (a Gauss-Seidel-type solver) which uses (ordered) backsubstitution, hence not suitable for parallel executions. The solution here was just to switch to a better suited solver such as the Conjugate-Gradient method. Acknowledgements Special thanks to Valentin Churavy for creating KernelAbstractions.jl and revising this article. And, of course, Gabriel D. Weymouth for creating WaterLily.jl and for helping in the revising of this article too! :)]]></summary></entry><entry><title type="html">From 3D to 2D turbulence in the wake of a circular cylinder</title><link href="/2023/02/27/span-effect-turbulence-nature.html" rel="alternate" type="text/html" title="From 3D to 2D turbulence in the wake of a circular cylinder" /><published>2023-02-27T14:00:00+01:00</published><updated>2023-02-27T14:00:00+01:00</updated><id>/2023/02/27/span-effect-turbulence-nature</id><content type="html" xml:base="/2023/02/27/span-effect-turbulence-nature.html"><![CDATA[<!--more-->
<p>In this post, I will briefly cover the work we published with my former PhD supervisors about the span effect of a circular cylinder on the turbulence nature of its wake.
The paper containing all the details is available <a href="https://arxiv.org/pdf/2008.08933">here</a>.</p>

<p>The main research question we tried to tackle was wether a critical cylinder span exists such that the wake turbulence statistics shift from 3D (classical -5/3 direct TKE spectra decay) to 2D (inverse energy cascade at -3 decay, or steeper).
And, if that was the case, why was that.</p>

<p>To do so, we conducted a series of simulations of incompressible flow past a circular cylinder at $Re=10^4$ with different span lengths, ranging from $L_z=10$ (span length non-dimensionalised with the cylinder diameter $D$) to pure 2-D simulations.
Below, an instantaneous snapshot of the vorticity field for each case is displayed.</p>

<p><img src="/assets/images/2023-27-02-span-effect-turbulence-nature/vorticity.svg" alt="image-title-here" class="img-responsive" /></p>

<p style="color:gray; font-size: 80%; text-align: center;">Figure 1. Instantaneous vorticity $\omega_z$ (red is positive, blue is negative) at the $z=L_z/2$ plane for: $(a)$ $L_z=0$, $(b)$ $L_z=0.1$, $(c)$ $L_z=0.25$, $(d)$ $L_z=0.5$, $(e)$ $L_z=1$, $(f)$ $L_z=\pi$.</p>

<p>It is easy to observe that, as the span is reduced, small vortices get rearranged into larger coherent structures which also contain more energy. This is a visual example of the inverse energy cascade commonly found in 2D fluid flow systems.</p>

<p>To quantify the turbulence nature of the wake, different analysis were done in terms of energy spectra, TKE spatial plots, lift and drag forces, separation points, among others.
The most striking results were observed for the two-point correlation plots, displayed in Figure 2 together with the energy spectra.
It shows that the correlation of the velocity $v$ component along the span always decays with increasing distance $(d)$, except when the span is larger than 1 diameter.
In this case, a local correlation maxima is found near $d/D=1$ which indicates the presence of a large-scale structure.
At this wavelength, the Mode-B structures of the circular cylinder wake, naturally present at $Re=10^4$, have enough room to develop in the spanwise direction, differently from the other cases.
At the same time, the spectra of the $L_z=\pi$ case is the only one displaying a purely -5/3 decaying TKE far from the cylinder (c), further assuring the presence of a 3D large-scale structure which is able to feed the inertial and viscous scales of the energy cascade.</p>

<p><img src="/assets/images/2023-27-02-span-effect-turbulence-nature/energy_spectas_and_two-points_correlations.svg" alt="image-title-here" class="img-responsive" /></p>

<p style="color:gray; font-size: 80%; text-align: center;">Figure 2.
Left: vertical velocity component temporal power spectra (PS) at different $(x, y)$ locations on the wake. (a): (2, 0.8), (b): (4, 0.8), (c): (8, 0.8).
The PS lines of each case are shifted a factor of 10 for clarity and the vertical axis ticks correspond to the $L_z=\pi$ case.
The dashed lines have a −11/3 slope and the dotted lines have a −5/3 slope.
Right: Two-point correlations along $z$ at the same $(x, y)$ locations as the left figures respectively.</p>

<p>To further assess this observation, the vertically-averaged TKE along the wake was computed as well as the lift coefficient, as displayed in Figure 3.
It can be observed that increasing the span from $L_z=\pi$ to $L_z=10$ did not yield significant differences in the TKE nor in the $C_L$.
On the other hand, the rest of the tested spans showed different results with increasing significance as the span was constricted.</p>

<p><img src="/assets/images/2023-27-02-span-effect-turbulence-nature/TKE_and_CL.svg" alt="image-title-here" class="img-responsive" /></p>

<p style="color:gray; font-size: 80%; text-align: center;">Figure 3.
Left: Total TKE along the wake, where the TKE is computed from the normal Reynolds stresses and averaged on the vertical direction.
Right: TKE to $C_L$
Note that, in both plots, there is no significant difference between $L_z=\pi$ and $L_z=10$.</p>

<p>To summarise, a span of $L_z=\pi$ was enough to fully capture the 3D turbulence behaviour while smaller spans led to somewhat two-dimensionalised results.
This effect was attributed to the fact that the $L_z=\pi$ case was the only case allowing for Mode-B wake structures to develop, which are naturally present at $Re=10^4$.
Hence, if you are going to perform scale-resolving simulations of this test case, be sure to have a sufficiently long span.</p>

<p>:wink:</p>]]></content><author><name>Bernat Font</name><email>bernatfontgarcia@gmail.com</email></author><category term="turbulence" /><category term="cylinder" /><category term="jfm" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Deep reinforcement learning for active flow control</title><link href="/2023/02/22/drl-circular-cylinder.html" rel="alternate" type="text/html" title="Deep reinforcement learning for active flow control" /><published>2023-02-22T14:00:00+01:00</published><updated>2023-02-22T14:00:00+01:00</updated><id>/2023/02/22/drl-circular-cylinder</id><content type="html" xml:base="/2023/02/22/drl-circular-cylinder.html"><![CDATA[<!--more-->

<p>Post under construction</p>

<p>:wink:</p>]]></content><author><name>Bernat Font</name><email>bernatfontgarcia@gmail.com</email></author><category term="drl" /><category term="cylinder" /><category term="drag" /><summary type="html"><![CDATA[]]></summary></entry></feed>